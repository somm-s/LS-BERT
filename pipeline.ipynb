{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_types = ['f', 'i']\n",
    "word_types = ['b', 'p']\n",
    "hp_filter_sizes = [0, 100, 150]\n",
    "burst_timeouts = [1000, 30000, 50000]\n",
    "sentence_timeouts = [500000, 1000000, 15000000]\n",
    "labeling_granularity = ['sentence', 'document', 'host']\n",
    "ngram_sizes = [2, 3, 4, 5]\n",
    "buckets = [5, 20, 50]\n",
    "percentiles = [0.95, 0.99]\n",
    "validation_year = 2022\n",
    "training_years = [2017, 2018, 2019, 2022]\n",
    "token_features = ['log_bytes', 'avg_bytes']\n",
    "log_maxs = [15, 16]\n",
    "avg_maxs = [1500, 2500]\n",
    "seq_lengths = [128, 512]\n",
    "doc_lengths = [100]\n",
    "embedding_lengths = [128, 256, 768]\n",
    "num_attention_heads = [4, 8, 12]\n",
    "num_hidden_layers = [4, 8, 12]\n",
    "\n",
    "sentence_type_id = 1\n",
    "word_type_id = 1\n",
    "hp_filter_size_id = 1\n",
    "burst_timeout_id = 1\n",
    "sentence_timeout_id = 2\n",
    "labeling_granularity_id = 1\n",
    "ngram_size_id = 1\n",
    "bucket_id = 2\n",
    "percentile_id = 0\n",
    "validation_year_id = 0\n",
    "training_year_ids = [0,1]\n",
    "token_feature_id = 0\n",
    "seq_length_id = 1\n",
    "doc_length_id = 0\n",
    "max_id = 0\n",
    "embedding_length_id = 1\n",
    "num_attention_heads_id = 1\n",
    "num_hidden_layers_id = 1\n",
    "\n",
    "filter_singletons = True\n",
    "do_training = True\n",
    "do_validation1 = True\n",
    "do_validation2 = True\n",
    "do_validation3 = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading year 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27045/27045 [00:01<00:00, 21397.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading year 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66815/66815 [00:02<00:00, 26307.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import tqdm\n",
    "\n",
    "def read_zip_archive(zip_file_path, contents, names, ips):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        for file_name in tqdm.tqdm(file_list):\n",
    "            if (not file_name.endswith('.csv')):\n",
    "                continue\n",
    "            name = file_name.split('/')[-1]\n",
    "            feature_name = file_name.split('/')[-2]\n",
    "            if (feature_name != token_features[token_feature_id]):\n",
    "                continue\n",
    "            name = name.split('.csv')[0]\n",
    "            ip1 = name.split('-')[0]\n",
    "            ip2 = name.split('-')[1]\n",
    "            names.append(name)\n",
    "            ips.add(ip1)\n",
    "            ips.add(ip2)\n",
    "            # Read the content of each file into a list of strings\n",
    "            with zip_ref.open(file_name) as file:\n",
    "                content = file.read().decode('utf-8')  # Assuming the content is in UTF-8 encoding\n",
    "                contents.append(content)\n",
    "    return contents, names, ips\n",
    "\n",
    "\n",
    "contents = []\n",
    "names = []\n",
    "ips = set()\n",
    "\n",
    "if word_types[word_type_id] == 'p':\n",
    "    burst_timeout = 'x'\n",
    "else:\n",
    "    burst_timeout = str(burst_timeouts[burst_timeout_id])\n",
    "\n",
    "\n",
    "for year_id in training_year_ids:\n",
    "    print('Reading year', training_years[year_id])\n",
    "    path = 'config_' + sentence_types[sentence_type_id] + word_types[word_type_id] + '_' + str(hp_filter_sizes[hp_filter_size_id]) + '_' + burst_timeout + '_' + str(sentence_timeouts[sentence_timeout_id]) + '.zip'\n",
    "    contents, names, ips = read_zip_archive(str(training_years[year_id]) + '/' + path, contents, names, ips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete singletons if flag is enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bucketization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46927/46927 [00:56<00:00, 836.77it/s] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_buckets = buckets[bucket_id]\n",
    "min_buckets = 0\n",
    "if token_features[token_feature_id] == 'log_bytes':\n",
    "    max_buckets = log_maxs[max_id]\n",
    "else:\n",
    "    max_buckets = avg_maxs[max_id]\n",
    "\n",
    "# Define the bucket boundaries\n",
    "bucket_boundaries = np.linspace(min_buckets, max_buckets, num_buckets)\n",
    "\n",
    "def bucket_id_from_decimal(decimal):\n",
    "    # Use np.digitize to find the bucket that each decimal belongs to\n",
    "    # 0 - 4 is reserved for special tokens\n",
    "    return np.digitize(decimal, bucket_boundaries) + 4\n",
    "\n",
    "corpus = []\n",
    "for content in tqdm.tqdm(contents):\n",
    "    document = []\n",
    "    corpus.append(document)\n",
    "    for sentence_id, row in enumerate(content.split('\\n')):\n",
    "        if sentence_id > doc_lengths[doc_length_id]:\n",
    "            break\n",
    "        sentence = []\n",
    "        for i, value in enumerate(row.split(' ')):\n",
    "            if i > seq_lengths[seq_length_id]:\n",
    "                break\n",
    "            if value == '':\n",
    "                continue\n",
    "            value = float(value)\n",
    "            sentence.append(bucket_id_from_decimal(value))\n",
    "        document.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preprocessing II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deduplicate sentences (e.g. only take first 100 sentences in each document)\n",
    "deduplicated_corpus = []\n",
    "for document in corpus:\n",
    "    document = document[:100]\n",
    "    deduplicated_corpus.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.shuffle(deduplicated_corpus)\n",
    "train_corpus, validation_corpus = train_test_split(deduplicated_corpus, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. BERT Preprocessing\n",
    "\n",
    "Special Tokens:\n",
    "- **0:** PAD token\n",
    "- **1:** CLS token (beginning of a sentence)\n",
    "- **2:** SEP token (end of sentence)\n",
    "- **3:** EMPTY token (empty sentence / end of document sentence)\n",
    "- **4:** MASK token for MLM objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42234 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42234/42234 [00:10<00:00, 4018.52it/s]\n",
      "100%|██████████| 4693/4693 [00:00<00:00, 83816.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from bert_dataset import BERTDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from bert_model import BERT, BERTLM\n",
    "from bert_trainer import BERTTrainer\n",
    "\n",
    "if do_training:\n",
    "    train_data = BERTDataset(train_corpus, seq_length=seq_lengths[seq_length_id], min_bucket=5, max_bucket=4 + num_buckets, augment=True)\n",
    "    # train_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=False)\n",
    "    eval_data = BERTDataset(validation_corpus, seq_length=seq_lengths[seq_length_id], min_bucket=5, max_bucket=4 + num_buckets)\n",
    "    # eval_loader = DataLoader(eval_data, batch_size=32, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 1, 31, 31, 39, 39, 37, 31, 29, 17, 32, 31, 31, 33, 32, 32, 31,  4, 32,\n",
       "          2,  4, 32, 39, 39,  4, 31, 31, 32, 22, 31, 31, 33,  2,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 1, 31, 31, 39, 39, 37, 31, 31, 32, 32, 31, 31, 33, 32, 32, 31, 31, 32,\n",
       "          2, 32, 32, 39, 39, 37, 31, 31, 32, 32, 31, 31, 33,  2,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'next_sentence_label': tensor(0)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertForPreTraining\n",
    "\n",
    "model_config = BertConfig(vocab_size=num_buckets + 5, \n",
    "                          max_position_embeddings=seq_lengths[seq_length_id],\n",
    "                          hidden_size=embedding_lengths[embedding_length_id],\n",
    "                          intermediate_size=4 * embedding_lengths[embedding_length_id],\n",
    "                          num_hidden_layers=8,\n",
    "                          num_attention_heads=8,)\n",
    "model = BertForPreTraining(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import os\n",
    "model_path = \"checkpoints\"\n",
    "folders = os.listdir(model_path)\n",
    "ids = [int(folder.split('_')[-1]) for folder in folders]\n",
    "max_id = max(ids) if ids else 0\n",
    "model_path = os.path.join(model_path, f\"model_{max_id + 1}\")\n",
    "try:\n",
    "    os.mkdir(model_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=3,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=32, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=128,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # do_eval=False,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23214' max='23214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23214/23214 2:23:26, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.135700</td>\n",
       "      <td>0.581934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.569300</td>\n",
       "      <td>0.534101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.534700</td>\n",
       "      <td>0.513446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.501120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.495093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.491159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.497200</td>\n",
       "      <td>0.483770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.494100</td>\n",
       "      <td>0.483563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.481723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.478434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.475302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.472576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.471104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.470719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.466815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.468874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.476300</td>\n",
       "      <td>0.469541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.476300</td>\n",
       "      <td>0.470020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.466877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.466372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.464248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.462979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23214, training_loss=0.5197988940505409, metrics={'train_runtime': 8606.5213, 'train_samples_per_second': 690.562, 'train_steps_per_second': 2.697, 'total_flos': 7393731414650880.0, 'train_loss': 0.5197988940505409, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "models_path = \"models\"\n",
    "\n",
    "# list all folders in models\n",
    "folders = os.listdir(models_path)\n",
    "\n",
    "# extract id in each folder\n",
    "ids = [int(folder.split('_')[-1]) for folder in folders]\n",
    "\n",
    "# get the maximum id\n",
    "max_id = max(ids) if ids else 0\n",
    "\n",
    "# get the evaluation loss\n",
    "eval_loss = result['eval_loss']\n",
    "\n",
    "# format eval_loss to first 3 decimal places\n",
    "eval_loss = \"{:.3f}\".format(eval_loss)\n",
    "\n",
    "dir_name = f\"model_{eval_loss}_{max_id + 1}\"\n",
    "path = os.path.join(models_path, dir_name)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# save the model\n",
    "trainer.save_model(path)\n",
    "\n",
    "import json\n",
    "# Assuming the hyperparameters are stored in a dictionary named `hyperparameters`\n",
    "hyperparameters = {\n",
    "    'sentence_type': sentence_types[sentence_type_id],\n",
    "    'word_type': word_types[word_type_id],\n",
    "    'hp_filter_size': hp_filter_sizes[hp_filter_size_id],\n",
    "    'burst_timeout': burst_timeouts[burst_timeout_id],\n",
    "    'sentence_timeout': sentence_timeouts[sentence_timeout_id],\n",
    "    'num_buckets': buckets[bucket_id],\n",
    "    'bucket_min': min_buckets,\n",
    "    'bucket_max': max_buckets,\n",
    "    'bucket_boundaries': bucket_boundaries.tolist(),\n",
    "    'seq_length': seq_lengths[seq_length_id],\n",
    "    'doc_length': doc_lengths[doc_length_id],\n",
    "    'embedding_length': embedding_lengths[embedding_length_id],\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 32,\n",
    "    'vocab_size': num_buckets + 5,\n",
    "    'max_position_embeddings': seq_lengths[seq_length_id],\n",
    "    'hidden_size': embedding_lengths[embedding_length_id],\n",
    "    'intermediate_size': 4 * embedding_lengths[embedding_length_id],\n",
    "    'num_hidden_layers': num_hidden_layers[num_hidden_layers_id],\n",
    "    'num_attention_heads': num_attention_heads[num_attention_heads_id],\n",
    "    'years': [training_years[year_id] for year_id in training_year_ids],\n",
    "    'feature': token_features[token_feature_id],\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 32,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'per_device_eval_batch_size': 128,\n",
    "    'logging_steps': 1000,\n",
    "    'save_steps': 1000,\n",
    "    'greater_is_better': False,\n",
    "    'metric_for_best_model': \"eval_loss\",\n",
    "    'load_best_model_at_end': True,\n",
    "    'save_total_limit': 3\n",
    "}\n",
    "\n",
    "hyperparameters['evaluation_result'] = result\n",
    "\n",
    "# Write to json file\n",
    "with open(os.path.join(path, 'hyperparameters.json'), 'w') as f:\n",
    "    json.dump(hyperparameters, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
