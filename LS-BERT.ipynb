{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"nb-10_ht-100000_di-uni_mn-0_mx-1500_lg-no\"\n",
    "dataset_path = \"datasets\"\n",
    "\n",
    "pre_train_prefixes = [\"2017\"]\n",
    "train_prefix = \"2017\"\n",
    "validation_prefix = \"2017\"\n",
    "output_dir = \"trained_models\"\n",
    "\n",
    "#### Pre-Processing:\n",
    "seq_length = 128\n",
    "deduplicate = True\n",
    "coalesce = True\n",
    "\n",
    "#### Pre-Training:\n",
    "do_pre_train = True\n",
    "pre_train_run_name = \"pre_train_\" + configuration\n",
    "per_device_batch_size = 64\n",
    "embedding_length = 256\n",
    "num_attention_heads = 8\n",
    "num_hidden_layers = 8\n",
    "num_epochs = 2\n",
    "\n",
    "#### Fine-Tuning:\n",
    "do_fine_tune = True\n",
    "fine_tune_run_name = \"fine_tune_\" + configuration\n",
    "num_fine_tune_epochs = 1\n",
    "fine_tune_per_device_batch_size = 128\n",
    "fine_tune_per_device_eval_batch_size = 256\n",
    "\n",
    "#### Evaluation:\n",
    "do_evaluate = True\n",
    "use_finetuned_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import *\n",
    "if do_pre_train:\n",
    "    pre_train_corpus = load_pretrain_data(pre_train_prefixes, configuration, dataset_path)\n",
    "if do_fine_tune or do_evaluate:\n",
    "    train_benign_corpus, train_rt_corpus = load_train_data(train_prefix, configuration, dataset_path)\n",
    "    valid_benign_corpus, valid_rt_corpus = load_train_data(validation_prefix, configuration, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "if do_pre_train:\n",
    "    pre_train_train_corpus, pre_train_test_corpus = train_test_split(pre_train_corpus, test_size=0.05, random_state=42)\n",
    "if do_fine_tune or do_evaluate:\n",
    "    train_benign_train_corpus, train_benign_test_corpus = train_test_split(train_benign_corpus, test_size=0.1, random_state=42)\n",
    "    train_rt_train_corpus, train_rt_test_corpus = train_test_split(train_rt_corpus, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Flow-Pair Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_combined_dataset import *\n",
    "if do_pre_train:\n",
    "    pre_train_train_data = FlowPairDataset(configuration, pre_train_train_corpus, seq_length=seq_length, deduplicate=deduplicate, coalesce=coalesce, shuffle=True)\n",
    "    pre_train_test_data = FlowPairDataset(configuration, pre_train_test_corpus, seq_length=seq_length, deduplicate=deduplicate, coalesce=coalesce, shuffle=True)\n",
    "if do_fine_tune or do_evaluate:\n",
    "    train_train_data = FlowPairDataset(configuration, train_rt_train_corpus, train_benign_train_corpus, seq_length=seq_length, deduplicate=True, coalesce=coalesce, shuffle=True, balanced=False)\n",
    "    train_test_data = FlowPairDataset(configuration, train_rt_test_corpus, train_benign_test_corpus, seq_length=seq_length, deduplicate=True, coalesce=coalesce, shuffle=True, balanced=False)\n",
    "    valid_valid_data = FlowPairDataset(configuration, valid_rt_corpus, valid_benign_corpus, seq_length=seq_length, deduplicate=True, coalesce=coalesce, shuffle=True, balanced=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_train_bert import *\n",
    "if do_pre_train:\n",
    "    pretrained_model_path = pre_train_bert(\n",
    "        run_name=pre_train_run_name,\n",
    "        config=configuration,\n",
    "        train_data=pre_train_train_data,\n",
    "        test_data=pre_train_test_data,\n",
    "        seq_length=seq_length,\n",
    "        num_epochs=num_epochs,\n",
    "        num_hidden_layers= num_hidden_layers,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        embedding_length=embedding_length,\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune_bert import *\n",
    "if do_fine_tune:\n",
    "    fine_tune_bert(\n",
    "        run_name=fine_tune_run_name,\n",
    "        config=configuration,\n",
    "        output_dir=output_dir,\n",
    "        train_data=train_train_data,\n",
    "        eval_data=valid_valid_data,\n",
    "        pretrain_run_name=pre_train_run_name,\n",
    "        per_device_train_batch_size=fine_tune_per_device_batch_size,\n",
    "        num_epochs=num_fine_tune_epochs,\n",
    "        per_device_eval_batch_size=fine_tune_per_device_eval_batch_size,\n",
    "        logging_steps=1,\n",
    "        save_steps=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_bert import *\n",
    "finetuned_model_path = os.path.join(output_dir, fine_tune_run_name, \"finetuned_model\")\n",
    "pretrained_model_path = os.path.join(output_dir, pre_train_run_name, \"pretrained_model\")\n",
    "if use_finetuned_model:\n",
    "    model_path = finetuned_model_path\n",
    "else:\n",
    "    model_path = pretrained_model_path\n",
    "    \n",
    "if do_evaluate:\n",
    "    train_data, train_labels = get_embeddings(model_path=model_path, data=train_train_data, resample=False)\n",
    "    test_data, test_labels = get_embeddings(model_path=model_path, data=train_test_data, resample=False)\n",
    "    valid_data, valid_labels = get_embeddings(model_path=model_path, data=valid_valid_data, resample=False)\n",
    "    \n",
    "    train_data_rt = train_data[train_labels == 0]\n",
    "    train_data_benign = train_data[train_labels == 1]\n",
    "    test_data_rt = test_data[test_labels == 0]\n",
    "    test_data_benign = test_data[test_labels == 1]\n",
    "    valid_data_rt = valid_data[valid_labels == 0]\n",
    "    valid_data_benign = valid_data[valid_labels == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer class label of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_bert import *\n",
    "pred_labels, pred_probas, true_labels = infer_label(fine_tuned_model_path=finetuned_model_path, data=valid_valid_data, batch_size=2048, use_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"Benign\", \"RT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_bert import *\n",
    "visualize_embeddings([valid_data_benign, valid_data_rt, test_data_rt, test_data_benign], [\"Valid_Benign\", \"Valid_RT\", \"Test RT\", \"Test benign\"], \"PCA Plot\", \"pca\", sample=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
